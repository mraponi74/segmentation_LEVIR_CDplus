{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tarea: Detección de cambios basado en Segmentación Semántica\n",
    "\n",
    "#### Dataset: LEVIR-CD+ [LEarning Vision and Remote sensing laboratory]\n",
    "\n",
    "* Tipo: No-Geoespacial,\n",
    "* Fuete: Google Earth, \n",
    "* muestras: 985, \n",
    "* clases: 2 (cambio=255 o no cambio=0), \n",
    "* tamaño imágenes: 1024x1024 pixeles\n",
    "* resolución: 50 cm, \n",
    "* 3 bandas espectrales (RGB),\n",
    "* 20 regiones urbanas diferentes en Texas, USA,\n",
    "* imágenes bi-temporales con span de 5 años (entre 2002 y 2020)\n",
    "\n",
    "https://justchenhao.github.io/LEVIR/\n",
    "\n",
    "https://arxiv.org/abs/2107.09244\n",
    "\n",
    "Modelo utilizado: Unet\n",
    "\n",
    "(fecha: 3-2-2023)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Importamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchgeo\n",
    "from torchgeo.datasets import LEVIRCDPlus\n",
    "from torchgeo.datasets.utils import unbind_samples\n",
    "from torchgeo.trainers import SemanticSegmentationTask\n",
    "from torchgeo.datamodules.utils import dataset_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "import kornia.augmentation as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifico si tengo GPU disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Creamos un directorio para guardar los checkpoint (*.ckpt) del entrenamiento de nuestra red neuronal (unet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"exp_1\"\n",
    "exp_dir = f\"Checkpoint/{exp_name}\"\n",
    "os.makedirs(exp_dir, exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Seteamos parámetros e hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "lr = 0.0001  #learning rate\n",
    "gpu_id = 0   #si tengo varias GPUs, selecciono la que quiero usar\n",
    "# device = torch.device(f\"cuda:{gpu_id}\")\n",
    "num_workers = 12\n",
    "patch_size = 256  #tamaño del crop (256x256 pixeles)\n",
    "val_split_pct = 0.2  #20% del conjunto de entrenamiento se separará para validación\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Descargamos el dataset (archivo zip), lo descomprimos y dividimos (split) en dos conjuntos:\n",
    "\n",
    "* train: 65% --> 637 imágenes\n",
    "* test: 35%  --> 348 imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train: 637 images\n",
      "test: 348 images\n"
     ]
    }
   ],
   "source": [
    "#los datos se alojarán en la carpeta \"LEVIRCDPlus\"\n",
    "train_dataset = LEVIRCDPlus(root=\"LEVIRCDPlus\", split=\"train\", download=True, checksum=True)\n",
    "test_dataset = LEVIRCDPlus(root=\"LEVIRCDPlus\", split=\"test\", download=True, checksum=True)\n",
    "\n",
    "print(f'train: {len(train_dataset)} images')\n",
    "print(f'test: {len(test_dataset)} images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Extenderemos la task `SemanticSegmentationTask` de TorchGeos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSemanticSegmentationTask(SemanticSegmentationTask):\n",
    "    \n",
    "    def plot(self, sample):\n",
    "        # sample[\"image\"] es un tensor de pytorch de tamaño (6, 256, 256)\n",
    "        # c/sample contiene 4 imágenes, 2 RGB y 2 B/W\n",
    "        image1 = sample[\"image\"][:3]\n",
    "        image2 = sample[\"image\"][3:]\n",
    "        mask = sample[\"mask\"]\n",
    "        prediction = sample[\"prediction\"]\n",
    "\n",
    "        #grafico las 4 imágenes y las visualizo con Tensorboard\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(4*5, 5))\n",
    "        axs[0].imshow(image1.permute(1,2,0)) #(3, 256, 256) --> (256, 256, 3)\n",
    "        axs[0].axis(\"off\")\n",
    "        axs[1].imshow(image2.permute(1,2,0))\n",
    "        axs[1].axis(\"off\")\n",
    "        axs[2].imshow(mask)  #(1024, 1024)\n",
    "        axs[2].axis(\"off\")\n",
    "        axs[3].imshow(prediction) #(1024, 1024)\n",
    "        axs[3].axis(\"off\")\n",
    "\n",
    "\n",
    "        axs[0].set_title(\"image 1\")\n",
    "        axs[1].set_title(\"image 2\")\n",
    "        axs[2].set_title(\"mask\")\n",
    "        axs[3].set_title(\"prediction\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        \"\"\"Computa y devuelve la training loss.\n",
    "        La diferencia entre este código y el original de SemanticSegmentationTask\n",
    "        es el uso de la función de ploteo\n",
    "\n",
    "        Args:\n",
    "            batch: la salida del DataLoader\n",
    "\n",
    "        Returns:\n",
    "            training loss\n",
    "        \"\"\"\n",
    "\n",
    "        batch = args[0]\n",
    "        batch_idx = args[1]\n",
    "\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"mask\"]\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat_hard = y_hat.argmax(dim=1)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        # Mientras entrenamos el modelo, vamos a registrar en un log los resultados del entrenamiento. \n",
    "        # En TensorBoard podremos ver la evolución de la función de pérdida de entrenamiento\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=False)\n",
    "        self.train_metrics(y_hat_hard , y)\n",
    "\n",
    "        if batch_idx < 10:\n",
    "            batch[\"prediction\"] = y_hat_hard\n",
    "            \n",
    "            for key in [\"image\", \"mask\", \"prediction\"]:\n",
    "                batch[key] = batch[key].cpu()\n",
    "            \n",
    "            sample = unbind_samples(batch)[0]\n",
    "\n",
    "            fig = self.plot(sample)\n",
    "            \n",
    "            summary_writer = self.logger.experiment\n",
    "            summary_writer.add_figure(f\"image/train/{batch_idx}\", fig, global_step = self.global_step)\n",
    "            # Visualizaremos en TensorBoard las todas las tulas de 4 imágenes \n",
    "            # (image1, image2, mask, prediction) de los primeros 10 batches.\n",
    "            # Se observa la evolución del aprendizaje del modelo (ver prediction) a lo largo los epochs\n",
    "                        \n",
    "            plt.close()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, *args, **kwargs):\n",
    "\n",
    "        \"\"\"Computa la pérdida para el conjunto de validación \n",
    "        y registra en un log las predicciones de ejemplo.\n",
    "\n",
    "        Args:\n",
    "            batch: la salida del DataLoader\n",
    "            batch_idx: el índice de este patch\n",
    "        \"\"\"\n",
    "\n",
    "        batch = args[0]\n",
    "        batch_idx = args[1]\n",
    "\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"mask\"]\n",
    "\n",
    "        y_hat = self.forward(x)\n",
    "        y_hat_hard = y_hat.argmax(dim=1)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.val_metrics(y_hat_hard, y)\n",
    "\n",
    "        #idem a trianing\n",
    "        if batch_idx < 10:\n",
    "            batch[\"prediction\"] = y_hat_hard\n",
    "            for key in [\"image\", \"mask\", \"prediction\"]:\n",
    "                batch[key] = batch[key].cpu()\n",
    "\n",
    "            sample = unbind_samples(batch)[0]\n",
    "            fig = self.plot(sample)\n",
    "            summary_writer = self.logger.experiment\n",
    "            summary_writer.add_figure(f\"image/val/{batch_idx}\", fig, global_step = self.global_step)\n",
    "            plt.close()\n",
    "            \n",
    "    def test_step(self, *args, **kwargs): #NEW from original\n",
    "        \"\"\"Computa la pérdida en el conjunto de testeo.\n",
    "\n",
    "        Args:\n",
    "            batch: la salida del DataLoader\n",
    "        \"\"\"\n",
    "        batch = args[0]\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"mask\"]\n",
    "        y_hat = self(x)\n",
    "        y_hat_hard = y_hat.argmax(dim=1)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "\n",
    "        # Los steps de test y validación sólo registran logs por c/epoch\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.test_metrics(y_hat_hard, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Creamos un nuevo `LightningDataModule` para el dataset LEVIR-CD+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LEVIRCDPlusDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=12,\n",
    "        num_workers=0,\n",
    "        val_split_pct=0.2,\n",
    "        patch_size=(256,256),\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.val_split_pct = val_split_pct\n",
    "        self.patch_size = patch_size\n",
    "        self.kwargs = kwargs\n",
    "    \n",
    "    def on_after_batch_transfer(self, batch, batch_idx):\n",
    "        if (\n",
    "            hasattr(self, \"trainer\")\n",
    "            and self.trainer is not None\n",
    "            and hasattr(self.trainer, \"training\")\n",
    "            and self.trainer.training\n",
    "        ):\n",
    "\n",
    "            #Kornia espera que masks tenga datos de tipo \"float\" con un canal de dimensión\n",
    "            x = batch[\"image\"]  #[12, 6, 1024, 1024]\n",
    "            y = batch[\"mask\"].float().unsqueeze(1)  #[12, 1024, 1024] --> [12, 1, 1024, 1024])\n",
    "\n",
    "            #Aplicamos Augmentations a nuestros datos usando la librería Kornia.\n",
    "            train_augmentations = K.AugmentationSequential(\n",
    "                K.RandomRotation(p=0.5, degrees=90),\n",
    "                K.RandomHorizontalFlip(p=0.5),\n",
    "                K.RandomVerticalFlip(p=0.5),\n",
    "                K.RandomCrop(self.patch_size),\n",
    "                K.RandomSharpness(p=0.5),\n",
    "                data_keys=[\"input\", \"mask\"],\n",
    "            )\n",
    "\n",
    "            x, y = train_augmentations(x, y)\n",
    "\n",
    "            #torchmetrics espera que masks tenga datos de tipo \"long\" sin un canal de dimensión\n",
    "            batch[\"image\"] = x\n",
    "            batch[\"mask\"] = y.squeeze(1).long()\n",
    "        \n",
    "        return batch\n",
    "    \n",
    "    def preprocess(self, sample):  \n",
    "        #normalizamos los datos de las imágenes, uint8 --> float [0, 1]\n",
    "        sample[\"image\"] = (sample[\"image\"]/255.0).float() #[2, 3, 1024, 1024]\n",
    "        sample[\"image\"] = torch.flatten(sample[\"image\"], 0, 1) #[6, 1024, 1024]\n",
    "        sample[\"mask\"] = sample[\"mask\"].long() #[1024, 1024]\n",
    "        \n",
    "        return sample\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        #Definimos los transformas de cada set de datos\n",
    "        train_transforms = Compose ([self.preprocess])\n",
    "        test_transforms = Compose ([self.preprocess])\n",
    "\n",
    "        #Aplicamos las transformaciones (preprocesado) a los datasets (train, val y test)\n",
    "        train_dataset = LEVIRCDPlus(\n",
    "            split=\"train\", transforms=train_transforms, **self.kwargs\n",
    "        )\n",
    "\n",
    "        if self.val_split_pct > 0.0:\n",
    "            #Hacemos el split de los datos de entrenamient\n",
    "            #80% train, 20% val\n",
    "            self.train_dataset, self.val_dataset, _ = dataset_split(\n",
    "                train_dataset, val_pct=self.val_split_pct, test_pct=0.0\n",
    "            )\n",
    "        else:\n",
    "            self.train_dataset = train_dataset\n",
    "            self.val_dataset = train_dataset\n",
    "        \n",
    "        self.test_dataset = LEVIRCDPlus(\n",
    "            split=\"test\", transforms=test_transforms, **self.kwargs\n",
    "        )\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Configuramos el entrenamiento:\n",
    "\n",
    "* Instanciamos el `datamodule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = LEVIRCDPlusDataModule(\n",
    "    root = \"LEVIRCDPlus\",\n",
    "    batch_size = batch_size,\n",
    "    num_workers = num_workers,\n",
    "    val_split_pct = val_split_pct,\n",
    "    patch_size = (patch_size, patch_size),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Instanciamos la tarea customizada: `CustomSemanticSegmentationTask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = CustomSemanticSegmentationTask(\n",
    "    model=\"unet\",\n",
    "    backbone=\"resnet18\",\n",
    "    weights=\"imagenet\",\n",
    "    in_channels=6,\n",
    "    num_classes=2,\n",
    "    loss=\"ce\",\n",
    "    ignore_index=None,\n",
    "    learning_rate=lr,\n",
    "    learning_rate_schedule_patience=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observación: se emplea el modelo `unet`, un backkbone basado en `resenet18`, se realizó un pre-entrenamiento usando el dataset `imagenet`, y se utiliza como función de pérdida la función cross-entropy (ce)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seteamos los callbacks y el logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=exp_dir,\n",
    "    save_top_k=1,\n",
    "    save_last=True,\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=0.00,\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "#La carpeta /logs/<name> contendrá los resultados para enviar a TensorBoard\n",
    "tb_logger = TensorBoardLogger(\n",
    "    save_dir=\"logs/\",\n",
    "    name=exp_name\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seteamos parámetros para el entrenador (`Trainer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    logger=[tb_logger],\n",
    "    default_root_dir=exp_dir,\n",
    "    min_epochs=1,\n",
    "    max_epochs=100,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_id]    \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Entrenamos el modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: logs/exp_1\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | model         | Unet             | 14.3 M\n",
      "1 | loss          | CrossEntropyLoss | 0     \n",
      "2 | train_metrics | MetricCollection | 0     \n",
      "3 | val_metrics   | MetricCollection | 0     \n",
      "4 | test_metrics  | MetricCollection | 0     \n",
      "---------------------------------------------------\n",
      "14.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "14.3 M    Total params\n",
      "57.351    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3325eadcabc64467b9c70c203d357d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e50ae7eec94a2d847ec2c3024c5a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72333fcadcaa4ec28786145b3c89234d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9184d7b1da9b42898d1015aa948ff762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a356431fc5c440a0ae11c8363f291931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e87728e1abd4f068ec4a9080e369f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f095bcfe4c534b5ab8c862b065bd010a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e2ae3c9e884235a2e0b6d60b54bd6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# torch.set_float32_matmul_precision('high')\n",
    "\n",
    "_ = trainer.fit(model=task, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model=task, datamodule=datamodule)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5c59cbd06cd4bc6070deb952d0cfe57d38e3f4165e8866b778ca5b2f937da71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
